{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3145: DtypeWarning: Columns (5,8,11,15) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "/usr/local/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3145: DtypeWarning: Columns (15) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n"
     ]
    }
   ],
   "source": [
    "trn = pd.read_csv('./data/train_ver2.csv')\n",
    "tst = pd.read_csv('./data/test_ver2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "prods = trn.columns[24:].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn[prods] = trn[prods].fillna(0.0).astype(np.int8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_product = trn[prods].sum(axis=1) == 0\n",
    "trn = trn[~no_product]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in trn.columns[24:]:\n",
    "    tst[col] = 0\n",
    "df = pd.concat([trn, tst], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_cols = ['ind_empleado', 'pais_residencia', 'sexo', 'tiprel_1mes', 'indresi', 'indext', 'conyuemp', 'canal_entrada', 'indfall', 'tipodom', 'nomprov', 'segmento']\n",
    "for col in categorical_cols:\n",
    "    df[col], _ = df[col].factorize(na_sentinel=-99)\n",
    "features += categorical_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['age'].replace(' NA', -99, inplace=True)\n",
    "df['age'] = df['age'].astype(np.int8)\n",
    "\n",
    "df['antiguedad'].replace('     NA', -99, inplace=True)\n",
    "df['antiguedad'] = df['antiguedad'].astype(np.int8)\n",
    "\n",
    "df['renta'].replace('         NA', -99, inplace=True)\n",
    "df['renta'].fillna(-99, inplace=True)\n",
    "df['renta'] = df['renta'].astype(float).astype(np.int8)\n",
    "\n",
    "df['indrel_1mes'].replace('P', 5, inplace=True)\n",
    "df['indrel_1mes'].fillna(-99, inplace=True)\n",
    "df['indrel_1mes'] = df['indrel_1mes'].astype(float).astype(np.int8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "features += ['age','antiguedad','renta','ind_nuevo','indrel','indrel_1mes','ind_actividad_cliente']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['fecha_alta_month'] = df['fecha_alta'].map(lambda x: 0.0 if x.__class__ is float else float(x.split('-')[1])).astype(np.int8)\n",
    "df['fecha_alta_year'] = df['fecha_alta'].map(lambda x: 0.0 if x.__class__ is float else float(x.split('-')[0])).astype(np.int16)\n",
    "features += ['fecha_alta_month', 'fecha_alta_year']\n",
    "\n",
    "df['ult_fec_cli_1t_month'] = df['ult_fec_cli_1t'].map(lambda x: 0.0 if x.__class__ is float else float(x.split('-')[1])).astype(np.int8)\n",
    "df['ult_fec_cli_1t_year'] = df['ult_fec_cli_1t'].map(lambda x: 0.0 if x.__class__ is float else float(x.split('-')[0])).astype(np.int16)\n",
    "features += ['ult_fec_cli_1t_month', 'ult_fec_cli_1t_year']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.fillna(-99, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "def date_to_int(str_date):\n",
    "    Y, M, D = [int(a) for a in str_date.strip().split(\"-\")] \n",
    "    int_date = (int(Y) - 2015) * 12 + int(M)\n",
    "    return int_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['int_date'] = df['fecha_dato'].map(date_to_int).astype(np.int8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lag = df.copy()\n",
    "df_lag.columns = [col + '_prev' if col not in ['ncodpers', 'int_date'] else col for col in df.columns ]\n",
    "df_lag['int_date'] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trn = df.merge(df_lag, on=['ncodpers','int_date'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df, df_lag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "for prod in prods:\n",
    "    prev = prod + '_prev'\n",
    "    df_trn[prev].fillna(0, inplace=True)\n",
    "df_trn.fillna(-99, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "features += [feature + '_prev' for feature in features]\n",
    "features += [prod + '_prev' for prod in prods]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_dates = ['2016-01-28', '2016-02-28', '2016-03-28', '2016-04-28', '2016-05-28']\n",
    "trn = df_trn[df_trn['fecha_dato'].isin(use_dates)]\n",
    "tst = df_trn[df_trn['fecha_dato'] == '2016-06-28']\n",
    "del df_trn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "Y = []\n",
    "for i, prod in enumerate(prods):\n",
    "    prev = prod + '_prev'\n",
    "    prX = trn[(trn[prod] == 1) & (trn[prev] == 0)]\n",
    "    prY = np.zeros(prX.shape[0], dtype=np.int8) + i\n",
    "    X.append(prX)\n",
    "    Y.append(prY)\n",
    "XY = pd.concat(X)\n",
    "Y = np.hstack(Y)\n",
    "XY['y'] = Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "vld_date = '2016-05-28'\n",
    "XY_trn = XY[XY['fecha_dato'] != vld_date]\n",
    "XY_vld = XY[XY['fecha_dato'] == vld_date]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = {\n",
    "    'booster': 'gbtree',\n",
    "    'max_depth': 8,\n",
    "    'nthread': 4,\n",
    "    'num_class': len(prods),\n",
    "    'objective': 'multi:softprob',\n",
    "    'silent': 1,\n",
    "    'eval_metric': 'mlogloss',\n",
    "    'eta': 0.1,\n",
    "    'min_child_weight': 10,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'colsample_bylevel': 0.9,\n",
    "    'seed': 2018,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_trn = XY_trn[features].values\n",
    "Y_trn = XY_trn['y'].values\n",
    "dtrn = xgb.DMatrix(X_trn, label=Y_trn, feature_names=features)\n",
    "\n",
    "X_vld = XY_vld[features].values\n",
    "Y_vld = XY_vld['y'].values\n",
    "dvld = xgb.DMatrix(X_vld, label=Y_vld, feature_names=features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11:00:04] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[0]\ttrain-mlogloss:2.68092\teval-mlogloss:2.68980\n",
      "[1]\ttrain-mlogloss:2.46791\teval-mlogloss:2.48087\n",
      "[2]\ttrain-mlogloss:2.29254\teval-mlogloss:2.30787\n",
      "[3]\ttrain-mlogloss:2.15647\teval-mlogloss:2.17447\n",
      "[4]\ttrain-mlogloss:2.04496\teval-mlogloss:2.06416\n",
      "[5]\ttrain-mlogloss:1.94649\teval-mlogloss:1.96528\n",
      "[6]\ttrain-mlogloss:1.86704\teval-mlogloss:1.88623\n",
      "[7]\ttrain-mlogloss:1.79857\teval-mlogloss:1.81833\n",
      "[8]\ttrain-mlogloss:1.73422\teval-mlogloss:1.75382\n",
      "[9]\ttrain-mlogloss:1.67965\teval-mlogloss:1.69974\n",
      "[10]\ttrain-mlogloss:1.62933\teval-mlogloss:1.64932\n",
      "[11]\ttrain-mlogloss:1.58643\teval-mlogloss:1.60678\n",
      "[12]\ttrain-mlogloss:1.54636\teval-mlogloss:1.56683\n",
      "[13]\ttrain-mlogloss:1.51154\teval-mlogloss:1.53227\n",
      "[14]\ttrain-mlogloss:1.47956\teval-mlogloss:1.50062\n",
      "[15]\ttrain-mlogloss:1.45154\teval-mlogloss:1.47324\n",
      "[16]\ttrain-mlogloss:1.42372\teval-mlogloss:1.44545\n",
      "[17]\ttrain-mlogloss:1.39950\teval-mlogloss:1.42146\n",
      "[18]\ttrain-mlogloss:1.37854\teval-mlogloss:1.40108\n",
      "[19]\ttrain-mlogloss:1.35748\teval-mlogloss:1.38014\n",
      "[20]\ttrain-mlogloss:1.33874\teval-mlogloss:1.36160\n",
      "[21]\ttrain-mlogloss:1.32145\teval-mlogloss:1.34485\n",
      "[22]\ttrain-mlogloss:1.30440\teval-mlogloss:1.32803\n",
      "[23]\ttrain-mlogloss:1.28899\teval-mlogloss:1.31281\n",
      "[24]\ttrain-mlogloss:1.27472\teval-mlogloss:1.29880\n",
      "[25]\ttrain-mlogloss:1.26147\teval-mlogloss:1.28594\n",
      "[26]\ttrain-mlogloss:1.24897\teval-mlogloss:1.27374\n",
      "[27]\ttrain-mlogloss:1.23805\teval-mlogloss:1.26317\n",
      "[28]\ttrain-mlogloss:1.22739\teval-mlogloss:1.25285\n",
      "[29]\ttrain-mlogloss:1.21739\teval-mlogloss:1.24328\n",
      "[30]\ttrain-mlogloss:1.20811\teval-mlogloss:1.23429\n",
      "[31]\ttrain-mlogloss:1.19976\teval-mlogloss:1.22637\n",
      "[32]\ttrain-mlogloss:1.19232\teval-mlogloss:1.21931\n",
      "[33]\ttrain-mlogloss:1.18486\teval-mlogloss:1.21221\n",
      "[34]\ttrain-mlogloss:1.17752\teval-mlogloss:1.20522\n",
      "[35]\ttrain-mlogloss:1.17085\teval-mlogloss:1.19895\n",
      "[36]\ttrain-mlogloss:1.16439\teval-mlogloss:1.19297\n",
      "[37]\ttrain-mlogloss:1.15858\teval-mlogloss:1.18773\n",
      "[38]\ttrain-mlogloss:1.15304\teval-mlogloss:1.18260\n",
      "[39]\ttrain-mlogloss:1.14783\teval-mlogloss:1.17782\n",
      "[40]\ttrain-mlogloss:1.14275\teval-mlogloss:1.17331\n",
      "[41]\ttrain-mlogloss:1.13820\teval-mlogloss:1.16918\n",
      "[42]\ttrain-mlogloss:1.13348\teval-mlogloss:1.16485\n",
      "[43]\ttrain-mlogloss:1.12936\teval-mlogloss:1.16118\n",
      "[44]\ttrain-mlogloss:1.12543\teval-mlogloss:1.15773\n",
      "[45]\ttrain-mlogloss:1.12154\teval-mlogloss:1.15417\n",
      "[46]\ttrain-mlogloss:1.11780\teval-mlogloss:1.15098\n",
      "[47]\ttrain-mlogloss:1.11419\teval-mlogloss:1.14769\n",
      "[48]\ttrain-mlogloss:1.11101\teval-mlogloss:1.14500\n",
      "[49]\ttrain-mlogloss:1.10777\teval-mlogloss:1.14218\n",
      "[50]\ttrain-mlogloss:1.10476\teval-mlogloss:1.13951\n",
      "[51]\ttrain-mlogloss:1.10165\teval-mlogloss:1.13685\n",
      "[52]\ttrain-mlogloss:1.09894\teval-mlogloss:1.13459\n",
      "[53]\ttrain-mlogloss:1.09625\teval-mlogloss:1.13243\n",
      "[54]\ttrain-mlogloss:1.09372\teval-mlogloss:1.13036\n",
      "[55]\ttrain-mlogloss:1.09144\teval-mlogloss:1.12866\n",
      "[56]\ttrain-mlogloss:1.08916\teval-mlogloss:1.12686\n",
      "[57]\ttrain-mlogloss:1.08699\teval-mlogloss:1.12513\n",
      "[58]\ttrain-mlogloss:1.08486\teval-mlogloss:1.12339\n",
      "[59]\ttrain-mlogloss:1.08278\teval-mlogloss:1.12174\n",
      "[60]\ttrain-mlogloss:1.08087\teval-mlogloss:1.12027\n",
      "[61]\ttrain-mlogloss:1.07893\teval-mlogloss:1.11876\n",
      "[62]\ttrain-mlogloss:1.07716\teval-mlogloss:1.11748\n",
      "[63]\ttrain-mlogloss:1.07554\teval-mlogloss:1.11625\n",
      "[64]\ttrain-mlogloss:1.07395\teval-mlogloss:1.11496\n",
      "[65]\ttrain-mlogloss:1.07235\teval-mlogloss:1.11378\n",
      "[66]\ttrain-mlogloss:1.07085\teval-mlogloss:1.11272\n",
      "[67]\ttrain-mlogloss:1.06940\teval-mlogloss:1.11163\n",
      "[68]\ttrain-mlogloss:1.06807\teval-mlogloss:1.11064\n",
      "[69]\ttrain-mlogloss:1.06665\teval-mlogloss:1.10963\n",
      "[70]\ttrain-mlogloss:1.06534\teval-mlogloss:1.10874\n",
      "[71]\ttrain-mlogloss:1.06418\teval-mlogloss:1.10796\n",
      "[72]\ttrain-mlogloss:1.06292\teval-mlogloss:1.10710\n",
      "[73]\ttrain-mlogloss:1.06166\teval-mlogloss:1.10625\n",
      "[74]\ttrain-mlogloss:1.06038\teval-mlogloss:1.10546\n",
      "[75]\ttrain-mlogloss:1.05921\teval-mlogloss:1.10471\n",
      "[76]\ttrain-mlogloss:1.05810\teval-mlogloss:1.10401\n",
      "[77]\ttrain-mlogloss:1.05700\teval-mlogloss:1.10336\n",
      "[78]\ttrain-mlogloss:1.05584\teval-mlogloss:1.10265\n",
      "[79]\ttrain-mlogloss:1.05483\teval-mlogloss:1.10209\n",
      "[80]\ttrain-mlogloss:1.05390\teval-mlogloss:1.10148\n",
      "[81]\ttrain-mlogloss:1.05284\teval-mlogloss:1.10092\n",
      "[82]\ttrain-mlogloss:1.05187\teval-mlogloss:1.10042\n",
      "[83]\ttrain-mlogloss:1.05088\teval-mlogloss:1.09996\n",
      "[84]\ttrain-mlogloss:1.04996\teval-mlogloss:1.09942\n",
      "[85]\ttrain-mlogloss:1.04898\teval-mlogloss:1.09895\n",
      "[86]\ttrain-mlogloss:1.04809\teval-mlogloss:1.09849\n",
      "[87]\ttrain-mlogloss:1.04719\teval-mlogloss:1.09804\n",
      "[88]\ttrain-mlogloss:1.04650\teval-mlogloss:1.09764\n",
      "[89]\ttrain-mlogloss:1.04566\teval-mlogloss:1.09719\n",
      "[90]\ttrain-mlogloss:1.04487\teval-mlogloss:1.09685\n",
      "[91]\ttrain-mlogloss:1.04409\teval-mlogloss:1.09651\n",
      "[92]\ttrain-mlogloss:1.04335\teval-mlogloss:1.09616\n",
      "[93]\ttrain-mlogloss:1.04262\teval-mlogloss:1.09583\n",
      "[94]\ttrain-mlogloss:1.04188\teval-mlogloss:1.09550\n",
      "[95]\ttrain-mlogloss:1.04109\teval-mlogloss:1.09516\n",
      "[96]\ttrain-mlogloss:1.04032\teval-mlogloss:1.09483\n",
      "[97]\ttrain-mlogloss:1.03967\teval-mlogloss:1.09461\n",
      "[98]\ttrain-mlogloss:1.03900\teval-mlogloss:1.09436\n",
      "[99]\ttrain-mlogloss:1.03825\teval-mlogloss:1.09415\n",
      "[100]\ttrain-mlogloss:1.03748\teval-mlogloss:1.09384\n",
      "[101]\ttrain-mlogloss:1.03674\teval-mlogloss:1.09357\n",
      "[102]\ttrain-mlogloss:1.03602\teval-mlogloss:1.09333\n",
      "[103]\ttrain-mlogloss:1.03514\teval-mlogloss:1.09314\n",
      "[104]\ttrain-mlogloss:1.03446\teval-mlogloss:1.09293\n",
      "[105]\ttrain-mlogloss:1.03375\teval-mlogloss:1.09268\n",
      "[106]\ttrain-mlogloss:1.03315\teval-mlogloss:1.09250\n",
      "[107]\ttrain-mlogloss:1.03251\teval-mlogloss:1.09237\n",
      "[108]\ttrain-mlogloss:1.03195\teval-mlogloss:1.09218\n",
      "[109]\ttrain-mlogloss:1.03137\teval-mlogloss:1.09204\n",
      "[110]\ttrain-mlogloss:1.03060\teval-mlogloss:1.09183\n",
      "[111]\ttrain-mlogloss:1.02995\teval-mlogloss:1.09163\n",
      "[112]\ttrain-mlogloss:1.02912\teval-mlogloss:1.09144\n",
      "[113]\ttrain-mlogloss:1.02844\teval-mlogloss:1.09126\n",
      "[114]\ttrain-mlogloss:1.02782\teval-mlogloss:1.09109\n",
      "[115]\ttrain-mlogloss:1.02715\teval-mlogloss:1.09092\n",
      "[116]\ttrain-mlogloss:1.02646\teval-mlogloss:1.09081\n",
      "[117]\ttrain-mlogloss:1.02591\teval-mlogloss:1.09071\n",
      "[118]\ttrain-mlogloss:1.02535\teval-mlogloss:1.09061\n",
      "[119]\ttrain-mlogloss:1.02464\teval-mlogloss:1.09055\n",
      "[120]\ttrain-mlogloss:1.02398\teval-mlogloss:1.09042\n",
      "[121]\ttrain-mlogloss:1.02330\teval-mlogloss:1.09034\n",
      "[122]\ttrain-mlogloss:1.02286\teval-mlogloss:1.09031\n",
      "[123]\ttrain-mlogloss:1.02222\teval-mlogloss:1.09019\n",
      "[124]\ttrain-mlogloss:1.02146\teval-mlogloss:1.09014\n",
      "[125]\ttrain-mlogloss:1.02088\teval-mlogloss:1.09002\n",
      "[126]\ttrain-mlogloss:1.02020\teval-mlogloss:1.08995\n",
      "[127]\ttrain-mlogloss:1.01963\teval-mlogloss:1.08984\n",
      "[128]\ttrain-mlogloss:1.01908\teval-mlogloss:1.08975\n",
      "[129]\ttrain-mlogloss:1.01855\teval-mlogloss:1.08968\n",
      "[130]\ttrain-mlogloss:1.01801\teval-mlogloss:1.08963\n",
      "[131]\ttrain-mlogloss:1.01753\teval-mlogloss:1.08956\n",
      "[132]\ttrain-mlogloss:1.01693\teval-mlogloss:1.08951\n",
      "[133]\ttrain-mlogloss:1.01629\teval-mlogloss:1.08939\n",
      "[134]\ttrain-mlogloss:1.01564\teval-mlogloss:1.08934\n",
      "[135]\ttrain-mlogloss:1.01511\teval-mlogloss:1.08927\n",
      "[136]\ttrain-mlogloss:1.01464\teval-mlogloss:1.08921\n",
      "[137]\ttrain-mlogloss:1.01400\teval-mlogloss:1.08917\n",
      "[138]\ttrain-mlogloss:1.01339\teval-mlogloss:1.08917\n",
      "[139]\ttrain-mlogloss:1.01283\teval-mlogloss:1.08913\n",
      "[140]\ttrain-mlogloss:1.01232\teval-mlogloss:1.08911\n",
      "[141]\ttrain-mlogloss:1.01173\teval-mlogloss:1.08901\n",
      "[142]\ttrain-mlogloss:1.01099\teval-mlogloss:1.08894\n",
      "[143]\ttrain-mlogloss:1.01047\teval-mlogloss:1.08887\n",
      "[144]\ttrain-mlogloss:1.00990\teval-mlogloss:1.08877\n",
      "[145]\ttrain-mlogloss:1.00920\teval-mlogloss:1.08866\n",
      "[146]\ttrain-mlogloss:1.00864\teval-mlogloss:1.08860\n",
      "[147]\ttrain-mlogloss:1.00809\teval-mlogloss:1.08848\n",
      "[148]\ttrain-mlogloss:1.00741\teval-mlogloss:1.08844\n",
      "[149]\ttrain-mlogloss:1.00678\teval-mlogloss:1.08838\n",
      "[150]\ttrain-mlogloss:1.00621\teval-mlogloss:1.08837\n",
      "[151]\ttrain-mlogloss:1.00571\teval-mlogloss:1.08834\n",
      "[152]\ttrain-mlogloss:1.00521\teval-mlogloss:1.08828\n",
      "[153]\ttrain-mlogloss:1.00464\teval-mlogloss:1.08827\n",
      "[154]\ttrain-mlogloss:1.00411\teval-mlogloss:1.08820\n",
      "[155]\ttrain-mlogloss:1.00355\teval-mlogloss:1.08816\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[156]\ttrain-mlogloss:1.00300\teval-mlogloss:1.08808\n",
      "[157]\ttrain-mlogloss:1.00243\teval-mlogloss:1.08807\n",
      "[158]\ttrain-mlogloss:1.00188\teval-mlogloss:1.08799\n",
      "[159]\ttrain-mlogloss:1.00129\teval-mlogloss:1.08798\n",
      "[160]\ttrain-mlogloss:1.00069\teval-mlogloss:1.08800\n",
      "[161]\ttrain-mlogloss:1.00002\teval-mlogloss:1.08800\n",
      "[162]\ttrain-mlogloss:0.99955\teval-mlogloss:1.08793\n",
      "[163]\ttrain-mlogloss:0.99911\teval-mlogloss:1.08795\n",
      "[164]\ttrain-mlogloss:0.99852\teval-mlogloss:1.08793\n",
      "[165]\ttrain-mlogloss:0.99795\teval-mlogloss:1.08789\n",
      "[166]\ttrain-mlogloss:0.99730\teval-mlogloss:1.08791\n",
      "[167]\ttrain-mlogloss:0.99680\teval-mlogloss:1.08786\n",
      "[168]\ttrain-mlogloss:0.99624\teval-mlogloss:1.08780\n",
      "[169]\ttrain-mlogloss:0.99573\teval-mlogloss:1.08776\n",
      "[170]\ttrain-mlogloss:0.99522\teval-mlogloss:1.08773\n",
      "[171]\ttrain-mlogloss:0.99476\teval-mlogloss:1.08767\n",
      "[172]\ttrain-mlogloss:0.99415\teval-mlogloss:1.08761\n",
      "[173]\ttrain-mlogloss:0.99358\teval-mlogloss:1.08763\n",
      "[174]\ttrain-mlogloss:0.99309\teval-mlogloss:1.08758\n",
      "[175]\ttrain-mlogloss:0.99254\teval-mlogloss:1.08758\n",
      "[176]\ttrain-mlogloss:0.99192\teval-mlogloss:1.08744\n",
      "[177]\ttrain-mlogloss:0.99126\teval-mlogloss:1.08737\n",
      "[178]\ttrain-mlogloss:0.99074\teval-mlogloss:1.08735\n",
      "[179]\ttrain-mlogloss:0.99012\teval-mlogloss:1.08738\n",
      "[180]\ttrain-mlogloss:0.98951\teval-mlogloss:1.08741\n",
      "[181]\ttrain-mlogloss:0.98893\teval-mlogloss:1.08742\n",
      "[182]\ttrain-mlogloss:0.98831\teval-mlogloss:1.08737\n",
      "[183]\ttrain-mlogloss:0.98772\teval-mlogloss:1.08732\n",
      "[184]\ttrain-mlogloss:0.98718\teval-mlogloss:1.08724\n",
      "[185]\ttrain-mlogloss:0.98664\teval-mlogloss:1.08732\n",
      "[186]\ttrain-mlogloss:0.98615\teval-mlogloss:1.08743\n",
      "[187]\ttrain-mlogloss:0.98555\teval-mlogloss:1.08742\n",
      "[188]\ttrain-mlogloss:0.98498\teval-mlogloss:1.08741\n",
      "[189]\ttrain-mlogloss:0.98442\teval-mlogloss:1.08738\n",
      "[190]\ttrain-mlogloss:0.98386\teval-mlogloss:1.08736\n",
      "[191]\ttrain-mlogloss:0.98317\teval-mlogloss:1.08742\n",
      "[192]\ttrain-mlogloss:0.98261\teval-mlogloss:1.08743\n",
      "[193]\ttrain-mlogloss:0.98216\teval-mlogloss:1.08741\n",
      "[194]\ttrain-mlogloss:0.98162\teval-mlogloss:1.08739\n",
      "[195]\ttrain-mlogloss:0.98115\teval-mlogloss:1.08740\n",
      "[196]\ttrain-mlogloss:0.98050\teval-mlogloss:1.08733\n",
      "[197]\ttrain-mlogloss:0.97987\teval-mlogloss:1.08728\n",
      "[198]\ttrain-mlogloss:0.97945\teval-mlogloss:1.08730\n",
      "[199]\ttrain-mlogloss:0.97897\teval-mlogloss:1.08729\n",
      "[200]\ttrain-mlogloss:0.97846\teval-mlogloss:1.08726\n",
      "[201]\ttrain-mlogloss:0.97795\teval-mlogloss:1.08730\n",
      "[202]\ttrain-mlogloss:0.97741\teval-mlogloss:1.08729\n",
      "[203]\ttrain-mlogloss:0.97678\teval-mlogloss:1.08729\n"
     ]
    }
   ],
   "source": [
    "watch_list = [(dtrn, 'train'), (dvld, 'eval')]\n",
    "model = xgb.train(param, dtrn, num_boost_round=1000, evals=watch_list, early_stopping_rounds=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(model, open(\"./model/xgb.baseline.pkl\", \"wb\"))\n",
    "best_ntree_limit = model.best_ntree_limit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "vld = trn[trn['fecha_dato'] == vld_date]\n",
    "ncodpers_vld = vld['ncodpers'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-237-fda838761a90>:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  vld[padd] = vld[prod] - vld[prev]\n"
     ]
    }
   ],
   "source": [
    "for prod in prods:\n",
    "    prev = prod + '_prev'\n",
    "    padd = prod + '_add'\n",
    "    vld[padd] = vld[prod] - vld[prev]    \n",
    "add_vld = vld[[prod + '_add' for prod in prods]].values\n",
    "add_vld_list = [list() for i in range(len(ncodpers_vld))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vld = 0\n",
    "for ncodper in range(len(ncodpers_vld)):\n",
    "    for prod in range(len(prods)):\n",
    "        if add_vld[ncodper, prod] > 0:\n",
    "            add_vld_list[ncodper].append(prod)\n",
    "            count_vld += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apk(actual, predicted, k=7, default=0.0):\n",
    "    # AP@7なので、最大7個まで使用します。\n",
    "    if len(predicted) > k:\n",
    "        predicted = predicted[:k]\n",
    "\n",
    "    score = 0.0\n",
    "    num_hits = 0.0\n",
    "\n",
    "    for i, p in enumerate(predicted):\n",
    "        # 点数を付与する条件は次のとおり :\n",
    "        # 予測値が正答に存在し (‘p in actual’)\n",
    "        # 予測値に重複がなければ (‘p not in predicted[:i]’) \n",
    "        if p in actual and p not in predicted[:i]:\n",
    "            num_hits += 1.0\n",
    "            score += num_hits / (i+1.0)\n",
    "\n",
    "    # 正答値が空白である場合、ともかく 0.0点を返します。\n",
    "    if not actual:\n",
    "        return default\n",
    "\n",
    "    # 正答の個数(len(actual))として average precisionを求めます。\n",
    "    return score / min(len(actual), k)\n",
    "\n",
    "def mapk(actual, predicted, k=7, default=0.0):\n",
    "    # list of listである正答値(actual)と予測値(predicted)から顧客別 Average Precisionを求め, np.mean()を通して平均を計算します。\n",
    "    return np.mean([apk(a, p, k, default) for a, p in zip(actual, predicted)]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.04266379915553903\n"
     ]
    }
   ],
   "source": [
    "print(mapk(add_vld_list, add_vld_list, 7, 0.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_all = XY[features].values\n",
    "Y_all = XY['y'].values\n",
    "dall = xgb.DMatrix(X_all, label=Y_all, feature_names=features)\n",
    "watch_list = [(dall, 'train')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_ntree_limit = int(best_ntree_limit * (len(XY_trn) + len(XY_vld)) / len(XY_trn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11:23:34] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[0]\ttrain-mlogloss:2.68170\n",
      "[1]\ttrain-mlogloss:2.46856\n",
      "[2]\ttrain-mlogloss:2.29314\n",
      "[3]\ttrain-mlogloss:2.15684\n",
      "[4]\ttrain-mlogloss:2.04526\n",
      "[5]\ttrain-mlogloss:1.94643\n",
      "[6]\ttrain-mlogloss:1.86679\n",
      "[7]\ttrain-mlogloss:1.79815\n",
      "[8]\ttrain-mlogloss:1.73355\n",
      "[9]\ttrain-mlogloss:1.67876\n",
      "[10]\ttrain-mlogloss:1.62828\n",
      "[11]\ttrain-mlogloss:1.58542\n",
      "[12]\ttrain-mlogloss:1.54514\n",
      "[13]\ttrain-mlogloss:1.51041\n",
      "[14]\ttrain-mlogloss:1.47838\n",
      "[15]\ttrain-mlogloss:1.45046\n",
      "[16]\ttrain-mlogloss:1.42268\n",
      "[17]\ttrain-mlogloss:1.39846\n",
      "[18]\ttrain-mlogloss:1.37757\n",
      "[19]\ttrain-mlogloss:1.35661\n",
      "[20]\ttrain-mlogloss:1.33794\n",
      "[21]\ttrain-mlogloss:1.32075\n",
      "[22]\ttrain-mlogloss:1.30379\n",
      "[23]\ttrain-mlogloss:1.28846\n",
      "[24]\ttrain-mlogloss:1.27418\n",
      "[25]\ttrain-mlogloss:1.26098\n",
      "[26]\ttrain-mlogloss:1.24860\n",
      "[27]\ttrain-mlogloss:1.23770\n",
      "[28]\ttrain-mlogloss:1.22702\n",
      "[29]\ttrain-mlogloss:1.21714\n",
      "[30]\ttrain-mlogloss:1.20794\n",
      "[31]\ttrain-mlogloss:1.19971\n",
      "[32]\ttrain-mlogloss:1.19226\n",
      "[33]\ttrain-mlogloss:1.18489\n",
      "[34]\ttrain-mlogloss:1.17753\n",
      "[35]\ttrain-mlogloss:1.17089\n",
      "[36]\ttrain-mlogloss:1.16453\n",
      "[37]\ttrain-mlogloss:1.15873\n",
      "[38]\ttrain-mlogloss:1.15330\n",
      "[39]\ttrain-mlogloss:1.14814\n",
      "[40]\ttrain-mlogloss:1.14315\n",
      "[41]\ttrain-mlogloss:1.13859\n",
      "[42]\ttrain-mlogloss:1.13390\n",
      "[43]\ttrain-mlogloss:1.12992\n",
      "[44]\ttrain-mlogloss:1.12603\n",
      "[45]\ttrain-mlogloss:1.12228\n",
      "[46]\ttrain-mlogloss:1.11853\n",
      "[47]\ttrain-mlogloss:1.11486\n",
      "[48]\ttrain-mlogloss:1.11180\n",
      "[49]\ttrain-mlogloss:1.10857\n",
      "[50]\ttrain-mlogloss:1.10563\n",
      "[51]\ttrain-mlogloss:1.10259\n",
      "[52]\ttrain-mlogloss:1.09995\n",
      "[53]\ttrain-mlogloss:1.09729\n",
      "[54]\ttrain-mlogloss:1.09477\n",
      "[55]\ttrain-mlogloss:1.09261\n",
      "[56]\ttrain-mlogloss:1.09032\n",
      "[57]\ttrain-mlogloss:1.08825\n",
      "[58]\ttrain-mlogloss:1.08618\n",
      "[59]\ttrain-mlogloss:1.08422\n",
      "[60]\ttrain-mlogloss:1.08238\n",
      "[61]\ttrain-mlogloss:1.08055\n",
      "[62]\ttrain-mlogloss:1.07888\n",
      "[63]\ttrain-mlogloss:1.07725\n",
      "[64]\ttrain-mlogloss:1.07564\n",
      "[65]\ttrain-mlogloss:1.07406\n",
      "[66]\ttrain-mlogloss:1.07258\n",
      "[67]\ttrain-mlogloss:1.07101\n",
      "[68]\ttrain-mlogloss:1.06963\n",
      "[69]\ttrain-mlogloss:1.06838\n",
      "[70]\ttrain-mlogloss:1.06706\n",
      "[71]\ttrain-mlogloss:1.06593\n",
      "[72]\ttrain-mlogloss:1.06469\n",
      "[73]\ttrain-mlogloss:1.06350\n",
      "[74]\ttrain-mlogloss:1.06239\n",
      "[75]\ttrain-mlogloss:1.06131\n",
      "[76]\ttrain-mlogloss:1.06024\n",
      "[77]\ttrain-mlogloss:1.05915\n",
      "[78]\ttrain-mlogloss:1.05808\n",
      "[79]\ttrain-mlogloss:1.05701\n",
      "[80]\ttrain-mlogloss:1.05617\n",
      "[81]\ttrain-mlogloss:1.05527\n",
      "[82]\ttrain-mlogloss:1.05445\n",
      "[83]\ttrain-mlogloss:1.05348\n",
      "[84]\ttrain-mlogloss:1.05254\n",
      "[85]\ttrain-mlogloss:1.05176\n",
      "[86]\ttrain-mlogloss:1.05093\n",
      "[87]\ttrain-mlogloss:1.05008\n",
      "[88]\ttrain-mlogloss:1.04926\n",
      "[89]\ttrain-mlogloss:1.04852\n",
      "[90]\ttrain-mlogloss:1.04769\n",
      "[91]\ttrain-mlogloss:1.04700\n",
      "[92]\ttrain-mlogloss:1.04626\n",
      "[93]\ttrain-mlogloss:1.04554\n",
      "[94]\ttrain-mlogloss:1.04491\n",
      "[95]\ttrain-mlogloss:1.04420\n",
      "[96]\ttrain-mlogloss:1.04354\n",
      "[97]\ttrain-mlogloss:1.04292\n",
      "[98]\ttrain-mlogloss:1.04231\n",
      "[99]\ttrain-mlogloss:1.04163\n",
      "[100]\ttrain-mlogloss:1.04099\n",
      "[101]\ttrain-mlogloss:1.04025\n",
      "[102]\ttrain-mlogloss:1.03953\n",
      "[103]\ttrain-mlogloss:1.03898\n",
      "[104]\ttrain-mlogloss:1.03842\n",
      "[105]\ttrain-mlogloss:1.03783\n",
      "[106]\ttrain-mlogloss:1.03731\n",
      "[107]\ttrain-mlogloss:1.03668\n",
      "[108]\ttrain-mlogloss:1.03626\n",
      "[109]\ttrain-mlogloss:1.03567\n",
      "[110]\ttrain-mlogloss:1.03507\n",
      "[111]\ttrain-mlogloss:1.03451\n",
      "[112]\ttrain-mlogloss:1.03390\n",
      "[113]\ttrain-mlogloss:1.03326\n",
      "[114]\ttrain-mlogloss:1.03278\n",
      "[115]\ttrain-mlogloss:1.03224\n",
      "[116]\ttrain-mlogloss:1.03156\n",
      "[117]\ttrain-mlogloss:1.03104\n",
      "[118]\ttrain-mlogloss:1.03045\n",
      "[119]\ttrain-mlogloss:1.02999\n",
      "[120]\ttrain-mlogloss:1.02943\n",
      "[121]\ttrain-mlogloss:1.02896\n",
      "[122]\ttrain-mlogloss:1.02850\n",
      "[123]\ttrain-mlogloss:1.02801\n",
      "[124]\ttrain-mlogloss:1.02750\n",
      "[125]\ttrain-mlogloss:1.02691\n",
      "[126]\ttrain-mlogloss:1.02636\n",
      "[127]\ttrain-mlogloss:1.02574\n",
      "[128]\ttrain-mlogloss:1.02518\n",
      "[129]\ttrain-mlogloss:1.02463\n",
      "[130]\ttrain-mlogloss:1.02407\n",
      "[131]\ttrain-mlogloss:1.02347\n",
      "[132]\ttrain-mlogloss:1.02309\n",
      "[133]\ttrain-mlogloss:1.02268\n",
      "[134]\ttrain-mlogloss:1.02218\n",
      "[135]\ttrain-mlogloss:1.02161\n",
      "[136]\ttrain-mlogloss:1.02097\n",
      "[137]\ttrain-mlogloss:1.02048\n",
      "[138]\ttrain-mlogloss:1.02005\n",
      "[139]\ttrain-mlogloss:1.01951\n",
      "[140]\ttrain-mlogloss:1.01909\n",
      "[141]\ttrain-mlogloss:1.01865\n",
      "[142]\ttrain-mlogloss:1.01808\n",
      "[143]\ttrain-mlogloss:1.01756\n",
      "[144]\ttrain-mlogloss:1.01711\n",
      "[145]\ttrain-mlogloss:1.01654\n",
      "[146]\ttrain-mlogloss:1.01617\n",
      "[147]\ttrain-mlogloss:1.01576\n",
      "[148]\ttrain-mlogloss:1.01521\n",
      "[149]\ttrain-mlogloss:1.01483\n",
      "[150]\ttrain-mlogloss:1.01431\n",
      "[151]\ttrain-mlogloss:1.01384\n",
      "[152]\ttrain-mlogloss:1.01332\n",
      "[153]\ttrain-mlogloss:1.01279\n",
      "[154]\ttrain-mlogloss:1.01228\n",
      "[155]\ttrain-mlogloss:1.01172\n",
      "[156]\ttrain-mlogloss:1.01121\n",
      "[157]\ttrain-mlogloss:1.01066\n",
      "[158]\ttrain-mlogloss:1.01005\n",
      "[159]\ttrain-mlogloss:1.00951\n",
      "[160]\ttrain-mlogloss:1.00911\n",
      "[161]\ttrain-mlogloss:1.00873\n",
      "[162]\ttrain-mlogloss:1.00813\n",
      "[163]\ttrain-mlogloss:1.00768\n",
      "[164]\ttrain-mlogloss:1.00713\n",
      "[165]\ttrain-mlogloss:1.00660\n",
      "[166]\ttrain-mlogloss:1.00619\n",
      "[167]\ttrain-mlogloss:1.00561\n",
      "[168]\ttrain-mlogloss:1.00513\n",
      "[169]\ttrain-mlogloss:1.00455\n",
      "[170]\ttrain-mlogloss:1.00401\n",
      "[171]\ttrain-mlogloss:1.00348\n",
      "[172]\ttrain-mlogloss:1.00287\n",
      "[173]\ttrain-mlogloss:1.00229\n",
      "[174]\ttrain-mlogloss:1.00190\n",
      "[175]\ttrain-mlogloss:1.00127\n",
      "[176]\ttrain-mlogloss:1.00067\n",
      "[177]\ttrain-mlogloss:1.00022\n",
      "[178]\ttrain-mlogloss:0.99976\n",
      "[179]\ttrain-mlogloss:0.99926\n",
      "[180]\ttrain-mlogloss:0.99883\n",
      "[181]\ttrain-mlogloss:0.99833\n",
      "[182]\ttrain-mlogloss:0.99779\n",
      "[183]\ttrain-mlogloss:0.99726\n",
      "[184]\ttrain-mlogloss:0.99681\n",
      "[185]\ttrain-mlogloss:0.99640\n",
      "[186]\ttrain-mlogloss:0.99602\n",
      "[187]\ttrain-mlogloss:0.99554\n",
      "[188]\ttrain-mlogloss:0.99519\n",
      "[189]\ttrain-mlogloss:0.99477\n",
      "[190]\ttrain-mlogloss:0.99431\n",
      "[191]\ttrain-mlogloss:0.99380\n",
      "[192]\ttrain-mlogloss:0.99335\n",
      "[193]\ttrain-mlogloss:0.99293\n",
      "[194]\ttrain-mlogloss:0.99247\n",
      "[195]\ttrain-mlogloss:0.99203\n",
      "[196]\ttrain-mlogloss:0.99151\n",
      "[197]\ttrain-mlogloss:0.99101\n",
      "[198]\ttrain-mlogloss:0.99054\n",
      "[199]\ttrain-mlogloss:0.99004\n",
      "[200]\ttrain-mlogloss:0.98963\n",
      "[201]\ttrain-mlogloss:0.98914\n",
      "[202]\ttrain-mlogloss:0.98873\n",
      "[203]\ttrain-mlogloss:0.98821\n",
      "[204]\ttrain-mlogloss:0.98762\n",
      "[205]\ttrain-mlogloss:0.98712\n",
      "[206]\ttrain-mlogloss:0.98669\n",
      "[207]\ttrain-mlogloss:0.98623\n",
      "[208]\ttrain-mlogloss:0.98572\n",
      "[209]\ttrain-mlogloss:0.98530\n",
      "[210]\ttrain-mlogloss:0.98495\n",
      "[211]\ttrain-mlogloss:0.98451\n",
      "[212]\ttrain-mlogloss:0.98403\n",
      "[213]\ttrain-mlogloss:0.98355\n",
      "[214]\ttrain-mlogloss:0.98305\n",
      "[215]\ttrain-mlogloss:0.98257\n",
      "[216]\ttrain-mlogloss:0.98211\n",
      "[217]\ttrain-mlogloss:0.98162\n",
      "[218]\ttrain-mlogloss:0.98104\n",
      "[219]\ttrain-mlogloss:0.98048\n",
      "[220]\ttrain-mlogloss:0.98013\n",
      "[221]\ttrain-mlogloss:0.97957\n",
      "[222]\ttrain-mlogloss:0.97922\n",
      "[223]\ttrain-mlogloss:0.97878\n",
      "[224]\ttrain-mlogloss:0.97827\n",
      "[225]\ttrain-mlogloss:0.97772\n",
      "[226]\ttrain-mlogloss:0.97715\n",
      "[227]\ttrain-mlogloss:0.97667\n"
     ]
    }
   ],
   "source": [
    "model = xgb.train(param, dall, num_boost_round=best_ntree_limit, evals=watch_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature importance:\n",
      "('renta', 19248)\n",
      "('age', 17869)\n",
      "('antiguedad', 17098)\n",
      "('age_prev', 12013)\n",
      "('antiguedad_prev', 11969)\n",
      "('fecha_alta_month', 11372)\n",
      "('nomprov', 10939)\n",
      "('fecha_alta_year', 8710)\n",
      "('renta_prev', 7744)\n",
      "('canal_entrada', 7244)\n",
      "('nomprov_prev', 5864)\n",
      "('canal_entrada_prev', 4331)\n",
      "('fecha_alta_month_prev', 3894)\n",
      "('ind_recibo_ult1_prev', 3285)\n",
      "('ind_ecue_fin_ult1_prev', 3062)\n",
      "('sexo', 3046)\n",
      "('fecha_alta_year_prev', 2971)\n",
      "('ind_cco_fin_ult1_prev', 2968)\n",
      "('ind_cno_fin_ult1_prev', 2724)\n",
      "('ind_tjcr_fin_ult1_prev', 2089)\n",
      "('segmento', 2084)\n",
      "('ind_reca_fin_ult1_prev', 1997)\n",
      "('segmento_prev', 1825)\n",
      "('ind_nom_pens_ult1_prev', 1650)\n",
      "('tiprel_1mes', 1562)\n",
      "('ind_valo_fin_ult1_prev', 1540)\n",
      "('ind_nomina_ult1_prev', 1437)\n",
      "('ind_ctop_fin_ult1_prev', 1433)\n",
      "('ind_dela_fin_ult1_prev', 1405)\n",
      "('ind_actividad_cliente', 1293)\n",
      "('sexo_prev', 1221)\n",
      "('tiprel_1mes_prev', 1102)\n",
      "('ind_ctpp_fin_ult1_prev', 1072)\n",
      "('ind_fond_fin_ult1_prev', 1001)\n",
      "('ind_ctma_fin_ult1_prev', 885)\n",
      "('ind_actividad_cliente_prev', 848)\n",
      "('indext', 780)\n",
      "('ind_nuevo', 697)\n",
      "('ind_plan_fin_ult1_prev', 626)\n",
      "('ind_hip_fin_ult1_prev', 548)\n",
      "('ind_nuevo_prev', 392)\n",
      "('indrel_1mes', 385)\n",
      "('ind_deco_fin_ult1_prev', 382)\n",
      "('indext_prev', 361)\n",
      "('pais_residencia', 215)\n",
      "('ind_empleado_prev', 182)\n",
      "('indrel', 168)\n",
      "('indrel_1mes_prev', 159)\n",
      "('ind_viv_fin_ult1_prev', 148)\n",
      "('ind_empleado', 141)\n",
      "('pais_residencia_prev', 130)\n",
      "('ind_ctju_fin_ult1_prev', 93)\n",
      "('ind_deme_fin_ult1_prev', 93)\n",
      "('ind_pres_fin_ult1_prev', 74)\n",
      "('ult_fec_cli_1t_month', 56)\n",
      "('indfall', 33)\n",
      "('ind_cder_fin_ult1_prev', 33)\n",
      "('indfall_prev', 22)\n",
      "('conyuemp_prev', 21)\n",
      "('indresi', 19)\n",
      "('ult_fec_cli_1t_year', 9)\n",
      "('conyuemp', 8)\n",
      "('indresi_prev', 4)\n"
     ]
    }
   ],
   "source": [
    "print(\"Feature importance:\")\n",
    "for kv in sorted([(k,v) for k,v in model.get_fscore().items()], key=lambda kv: kv[1], reverse=True):\n",
    "    print(kv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tst = tst[features].values\n",
    "dtst = xgb.DMatrix(X_tst, feature_names=features)\n",
    "preds_tst = model.predict(dtst, ntree_limit=best_ntree_limit)\n",
    "ncodpers_tst = tst['ncodpers'].values\n",
    "preds_tst = preds_tst - tst[[prod + '_prev' for prod in prods]].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit_file = open('./model/xgb.baseline.2015-06-28', 'w')\n",
    "submit_file.write('ncodpers,added_products\\n')\n",
    "for ncodper, pred in zip(ncodpers_tst, preds_tst):\n",
    "    y_prods = [(y,p,ip) for y,p,ip in zip(pred, prods, range(len(prods)))]\n",
    "    y_prods = sorted(y_prods, key=lambda a: a[0], reverse=True)[:7]\n",
    "    y_prods = [p for y,p,ip in y_prods]\n",
    "    submit_file.write('{},{}\\n'.format(int(ncodper), ' '.join(y_prods)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
